{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b170d284-5223-4390-84bc-17797259e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import GPT4All, OpenAI\n",
    "from langchain.callbacks.streaming_stdout_final_only import \\\n",
    "    FinalStreamingStdOutCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93fed3e5-149a-46ae-998c-211ba2df05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReviewGeneration(product_name, input_criteria_score : dict) -> None:\n",
    "    \"\"\"\n",
    "    This function expects a dictionary of the form:\n",
    "    input_criteria_score = [{\"criteria_0\":score_0}, {\"criteria_1\":score_1}, ...., {\"criteria_n\":score_n}]\n",
    "    if not, this function returns None.\n",
    "    \"\"\"\n",
    "    # Global Variables\n",
    "    N = len(input_criteria_score[0]) #number of criteria\n",
    "\n",
    "    # transforming score into adjectives\n",
    "    criteria = [{\"0\":\"Very bad\", \"1\":\"Bad\", \"2\":\"Not so good\", \"3\":\"Acceptable\", \"4\":\"Good\", \"5\":\"Very good\"}]\n",
    "\n",
    "    # Decompressing input_criteria_score dictionary\n",
    "    prefix = []\n",
    "    input_criteria = []\n",
    "    input_adjectives = []\n",
    "    for key, value in input_criteria_score[0].items():\n",
    "        input_criteria.append(key)\n",
    "        input_adjectives.append(criteria[0][str(input_criteria_score[0][key])])\n",
    "        pref = f\"{key}:{criteria[0][str(input_criteria_score[0][key])]}\"\n",
    "        prefix.append(pref)\n",
    "\n",
    "    # Criteria, Adjective and Feeling lists\n",
    "    criteria_list, adjective_list, feeling_list = [f\"criteria_{n}\" for n in range(N)],\\\n",
    "                                                  [f\"adjective_{n}\" for n in range(N)],\\\n",
    "                                                  [f\"feeling_{n}\" for n in range(N)]\n",
    "\n",
    "       \n",
    "    # template for PromptTemplate()\n",
    "    def template(prefix):\n",
    "        \"\"\"\n",
    "            This function build a template with a set of instructions to be passed to the model.\n",
    "        \"\"\"\n",
    "        temp = f\" You are a costumer. I want you to generate a product review on {product_name}\\\n",
    "               considering the following criteria: {prefix}. Be precise and concise.\"\n",
    "\n",
    "        return temp\n",
    "    \n",
    "    template = template(prefix)\n",
    "\n",
    "    #input variables for PromptTemplate()\n",
    "    def input_variable(product_name, criteria_list,  adjective_list,  feeling_list=None,  **kwargs):\n",
    "        \"\"\"\n",
    "            This function builds the input_variables necessary for the PromptTemplate()\n",
    "            Here, we have made feeling_list optional. input_variable() can be called without it!\n",
    "        \"\"\"\n",
    "        if feeling_list is not None:\n",
    "            input_var = [f\"{product_name}\"] + criteria_list + adjective_list + feeling_list\n",
    "        else:\n",
    "            input_var = [f\"{product_name}\"] + criteria_list + adjective_list\n",
    "\n",
    "        return input_var\n",
    "\n",
    "    #instantiating input_variable()\n",
    "    input_variables = input_variable(product_name, criteria_list,  adjective_list)\n",
    "\n",
    "    #instantiating the model\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "\n",
    "    # building the prompt\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables = input_variables,\n",
    "        template = template\n",
    "            )\n",
    "    def Prompt(**kwargs):\n",
    "        if N == 1:\n",
    "            response = llm(prompt.format(criteria_0=input_criteria[0], adjective_0=input_adjectives[0])                  \n",
    "                          )\n",
    "        if N == 2:\n",
    "            response = llm(prompt.format(criteria_0=input_criteria[0], criteria_1=input_criteria[1],\n",
    "                          adjective_0=input_adjectives[0], adjective_1=input_adjectives[1])                          \n",
    "                          )\n",
    "        if N == 3:\n",
    "            response = llm(prompt.format(criteria_0=input_criteria[0], criteria_1=input_criteria[1], criteria_2=input_criteria[2],\\\n",
    "                          adjective_0=input_adjectives[0], adjective_1=input_adjectives[1], adjective_2=input_adjectives[2])                          \n",
    "                          )\n",
    "        if N == 4:\n",
    "            response = llm(prompt.format(criteria_0=input_criteria[0], criteria_1=input_criteria[1], criteria_2=input_criteria[2], criteria_3=input_criteria[3],\\\n",
    "                          adjective_0=input_adjectives[0], adjective_1=input_adjectives[1], adjective_2=input_adjectives[2], adjective_3=input_adjectives[3])                          \n",
    "                          )\n",
    "        if N == 5:\n",
    "            response = llm(prompt.format(criteria_0=input_criteria[0], criteria_1=input_criteria[1], criteria_2=input_criteria[2], criteria_3=input_criteria[3], criteria_4=input_criteria[4],\\\n",
    "                          adjective_0=input_adjectives[0], adjective_1=input_adjectives[1], adjective_2=input_adjectives[2], adjective_3=input_adjectives[3], adjective_4=input_adjectives[4])                          \n",
    "                          )\n",
    "        \n",
    "        return response\n",
    "\n",
    "    # review_generation = Prompt()\n",
    "    return 1 #review_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7782cc85-c7ef-498f-b1ca-cb95651b3526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "product_name = \"iphone\"\n",
    "\n",
    "#fake values\n",
    "score = [0, 4, 2, 3, 1]\n",
    "criteria_score = [{\"Design and Build Quality\":score[0], \"Display\":score[1], \"Performance\":score[2], \"Camera Quality\":score[3], \"Battery Life\":score[4]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ac5ce7d-a744-4a77-b53c-0a96787e0391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI recently purchased the iPhone and I must say, I am quite disappointed with the design and build quality. It feels flimsy and cheap, definitely not worth the price I paid for it. However, the display is good and the colors are vibrant, so that's a plus.\\n\\nWhen it comes to performance, I have to say it's not so good. Apps often freeze and it takes forever to load anything. This can be quite frustrating, especially when I need to get things done quickly.\\n\\nThe camera quality is acceptable, but nothing to write home about. It takes decent pictures in good lighting, but the photos tend to be grainy in low light situations. As for battery life, it's bad. I find myself constantly needing to charge my phone throughout the day, which can be a hassle.\\n\\nIn conclusion, while the iPhone may have a good display and acceptable camera quality, the overall design and build quality, performance, and battery life do not meet my expectations. I am disappointed with my purchase and would not recommend it to others.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewGeneration(product_name, criteria_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c44e2354-5a01-4dbf-8129-7f550aa233b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "product_name = \"jean\"\n",
    "\n",
    "score = [4, 3, 2, 3, 0]\n",
    "criteria_score = [{\n",
    "        \"Design\": score[0],\n",
    "        \"Comfort\": score[1],\n",
    "        \"Durability\": score[2],\n",
    "        \"Fit\": score[3],\n",
    "        \"Price\": score[4]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbacc51d-0e9d-4d90-bd30-37259f66e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI recently purchased a pair of jeans and after wearing them for a while, I wanted to share my experience with others. Overall, I found that the jeans had a good design, acceptable comfort, and an acceptable fit. However, I was disappointed with the durability and the price of the jeans.\\n\\nStarting with the positives, I really liked the design of the jeans. The color and style were exactly what I was looking for and I received many compliments when wearing them. The comfort level was also acceptable, though not exceptional. The fabric was soft and had some stretch, but after a few hours of wear, I did find myself wanting to take them off.\\n\\nIn terms of fit, I found the jeans to be acceptable. They fit well in the waist and hips, but I did notice some bagginess in the thighs. However, overall, I was happy with how they looked on me.\\n\\nUnfortunately, the durability of the jeans was not up to my expectations. After only a few wears, I started to notice some fraying in the seams and the fabric was not holding up as well as I had hoped. This was disappointing, especially considering the price of the jeans.\\n\\nWhich brings me to my biggest issue with these jeans - the price. I personally feel that the price'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewGeneration(product_name, criteria_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cccbd48c-0a36-4e3f-8d9e-897db8dc8a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewGeneration(product_name, criteria_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a502bf8-ac71-4b82-b230-172a0d4a84a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kenmoe/code/jo25425/review-assistant/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d76649b4-0946-43a3-a2d5-7c64af3e7d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4ALL_MODELS_URL = \"https://gpt4all.io/models/gguf/\"\n",
    "\n",
    "def download_gpt4all_model(model_name: str, model_path: str):\n",
    "    url = GPT4ALL_MODELS_URL + model_name\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    with open(model_path, mode='wb') as out:\n",
    "        for chunk in response.iter_content(chunk_size=10 * 1024):\n",
    "            out.write(chunk)\n",
    "\n",
    "def load_model_review(mode: str, model_name: str):\n",
    "\n",
    "    # OpenAI\n",
    "    if mode == 'openai':\n",
    "        llm = OpenAI(model_name=model_name, openai_api_key=OPENAI_API_KEY, temperature=0.9)\n",
    "        return llm\n",
    "\n",
    "    # Local\n",
    "    if mode == 'local':\n",
    "        model_name = \"mistral-7b-openorca.gguf2.Q4_0.gguf\"\n",
    "        MODEL_DIR = \"./models\"\n",
    "        #model_path = os.path.join(MODEL_DIR, model_name)\n",
    "    \n",
    "        model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "        if not os.path.exists(model_path) \\\n",
    "            and os.path.isfile(model_path):\n",
    "            download_gpt4all_model(model_name, model_path)\n",
    "\n",
    "        callbacks = [FinalStreamingStdOutCallbackHandler()]\n",
    "        llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)\n",
    "        return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0be56f75-67a4-4c20-a54a-db0e3d2a2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "def ReviewGeneration(product_name, input_criteria_score : dict) -> None:\n",
    "    \"\"\"\n",
    "    This function expects a dictionary of the form:\n",
    "    input_criteria_score = [{\"criteria_0\":score_0}, {\"criteria_1\":score_1}, ...., {\"criteria_n\":score_n}]\n",
    "    if not, this function returns None.\n",
    "    \"\"\"\n",
    "    # Global Variables\n",
    "    N = len(input_criteria_score[0]) #number of criteria\n",
    "\n",
    "    # transforming score into adjectives\n",
    "    criteria = [{\"0\":\"Very bad\", \"1\":\"Bad\", \"2\":\"Not so good\", \"3\":\"Acceptable\", \"4\":\"Good\", \"5\":\"Very good\"}]\n",
    "\n",
    "    # Decompressing input_criteria_score dictionary\n",
    "    prefix = []\n",
    "    input_criteria = []\n",
    "    input_adjectives = []\n",
    "    for key, value in input_criteria_score[0].items():\n",
    "        input_criteria.append(key)\n",
    "        input_adjectives.append(criteria[0][str(input_criteria_score[0][key])])\n",
    "        pref = f\"{key}:{criteria[0][str(input_criteria_score[0][key])]}\"\n",
    "        prefix.append(pref)\n",
    "\n",
    "    # Criteria, Adjective and Feeling lists\n",
    "    criteria_list, adjective_list, feeling_list = [f\"criteria_{n}\" for n in range(N)],\\\n",
    "                                                  [f\"adjective_{n}\" for n in range(N)],\\\n",
    "                                                  [f\"feeling_{n}\" for n in range(N)]\n",
    "\n",
    "       \n",
    "    # template for PromptTemplate()\n",
    "    def template(prefix):\n",
    "        \"\"\"\n",
    "            This function builds a template with a set of instructions to be passed to the model.\n",
    "        \"\"\"\n",
    "        temp = f\" You are a costumer. I want you to generate a product review on {product_name}\\\n",
    "               considering the following criteria: {prefix}. Be precise and concise.\"\n",
    "\n",
    "        return temp\n",
    "\n",
    "    #instantiating the model\n",
    "    model_name = \"mistral-7b-openorca.gguf2.Q4_0.gguf\"\n",
    "    mode = \"local\"\n",
    "    #MODEL_DIR = \"./models\"\n",
    "    #model_path = os.path.join(MODEL_DIR, model_name)\n",
    "    \n",
    "    template = template(prefix)\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    llm = load_model_review(mode, model_name)\n",
    "\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "    review_generation = llm_chain.run(question)\n",
    "    \n",
    "    return review_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03c5c702-3f1a-492e-b9e9-30dc8817f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "product_name = \"jean\"\n",
    "\n",
    "score = [4, 3, 2, 3, 0]\n",
    "criteria_score = [{\n",
    "        \"Design\": score[0],\n",
    "        \"Comfort\": score[1],\n",
    "        \"Durability\": score[2],\n",
    "        \"Fit\": score[3],\n",
    "        \"Price\": score[4]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99083a3f-3697-4f7b-94e0-d2bbc902f05f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GPT4All\n__root__\n  cannot inherit from both a TypedDict type and a non-TypedDict base class (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mReviewGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriteria_score\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 49\u001b[0m, in \u001b[0;36mReviewGeneration\u001b[0;34m(product_name, input_criteria_score)\u001b[0m\n\u001b[1;32m     45\u001b[0m template \u001b[38;5;241m=\u001b[39m template(prefix)\n\u001b[1;32m     47\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n\u001b[0;32m---> 49\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_review\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(prompt\u001b[38;5;241m=\u001b[39mprompt, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m     53\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat NFL team won the Super Bowl in the year Justin Bieber was born?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[51], line 31\u001b[0m, in \u001b[0;36mload_model_review\u001b[0;34m(mode, model_name)\u001b[0m\n\u001b[1;32m     28\u001b[0m     download_gpt4all_model(model_name, model_path)\n\u001b[1;32m     30\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [FinalStreamingStdOutCallbackHandler()]\n\u001b[0;32m---> 31\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llm\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/review-assistant/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/review-assistant/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GPT4All\n__root__\n  cannot inherit from both a TypedDict type and a non-TypedDict base class (type=type_error)"
     ]
    }
   ],
   "source": [
    "ReviewGeneration(product_name, criteria_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eca7686c-e956-418b-b261-ee431051c515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kenmoe/code/jo25425/review-assistant/notebooks\n"
     ]
    }
   ],
   "source": [
    "https://python.langchain.com/docs/integrations/llms/openai\n",
    "https://python.langchain.com/docs/integrations/llms/gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcf5afae-6a72-4a5b-813e-0d4534dcd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./models/mistral-7b-openorca.gguf2.Q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de4ac86-4955-4967-a0e1-d2ee41860061",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GPT4All\n__root__\n  cannot inherit from both a TypedDict type and a non-TypedDict base class (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [StreamingStdOutCallbackHandler()]\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mGPT4All\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/review-assistant/lib/python3.10/site-packages/langchain_core/load/serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/review-assistant/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for GPT4All\n__root__\n  cannot inherit from both a TypedDict type and a non-TypedDict base class (type=type_error)"
     ]
    }
   ],
   "source": [
    "\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "384cb775-7cda-4a44-890d-53a828169917",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m GPT4All(model\u001b[38;5;241m=\u001b[39m\u001b[43mlocal_path\u001b[49m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m GPT4All(model\u001b[38;5;241m=\u001b[39mlocal_path, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptj\u001b[39m\u001b[38;5;124m\"\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_path' is not defined"
     ]
    }
   ],
   "source": [
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n",
    "\n",
    "llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e7681da-d335-4123-9177-1ebf38d2e0ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LLMChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m(prompt\u001b[38;5;241m=\u001b[39mprompt, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat NFL team won the Super Bowl in the year Justin Bieber was born?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m llm_chain\u001b[38;5;241m.\u001b[39mrun(question)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LLMChain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d415c99-5c93-4150-a1d4-d990f223abca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
