{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba92243a-254c-4b27-8e08-c9171cc618f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T23:05:45.066570Z",
     "iopub.status.busy": "2024-03-21T23:05:45.065842Z",
     "iopub.status.idle": "2024-03-21T23:05:45.094372Z",
     "shell.execute_reply": "2024-03-21T23:05:45.093625Z",
     "shell.execute_reply.started": "2024-03-21T23:05:45.066519Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b2c5e6e-cd43-4524-8833-7c19b238a53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T23:44:06.741101Z",
     "iopub.status.busy": "2024-03-21T23:44:06.740146Z",
     "iopub.status.idle": "2024-03-21T23:44:06.772226Z",
     "shell.execute_reply": "2024-03-21T23:44:06.771474Z",
     "shell.execute_reply.started": "2024-03-21T23:44:06.741052Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "770d2aab-146d-4aba-8ba8-6527de5c65b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:00:34.654564Z",
     "iopub.status.busy": "2024-03-22T00:00:34.653627Z",
     "iopub.status.idle": "2024-03-22T00:00:34.724484Z",
     "shell.execute_reply": "2024-03-22T00:00:34.723098Z",
     "shell.execute_reply.started": "2024-03-22T00:00:34.654512Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_text = \"Given this product title, please select between 3 and 6 criteria to rate in order to compose a product review. No need to explain the criteria.\"\n",
    "product_text = 'Maybelline Instant Age Rewind Eraser Dark Circles Treatment Concealer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7ace0-fb40-4fb4-8e1f-023cd876aa68",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9878adc-0099-48a8-abcd-edd8b00c23a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T23:44:32.252451Z",
     "iopub.status.busy": "2024-03-21T23:44:32.251830Z",
     "iopub.status.idle": "2024-03-21T23:44:32.282545Z",
     "shell.execute_reply": "2024-03-21T23:44:32.281589Z",
     "shell.execute_reply.started": "2024-03-21T23:44:32.252406Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "def get_text_chunks(text) -> list[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    docs = [Document(page_content=x) for x in text_splitter.split_text(text)]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ca535f7-3086-4d13-be2b-8e036859c1a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:54:47.170216Z",
     "iopub.status.busy": "2024-03-22T00:54:47.169739Z",
     "iopub.status.idle": "2024-03-22T00:54:47.240837Z",
     "shell.execute_reply": "2024-03-22T00:54:47.239737Z",
     "shell.execute_reply.started": "2024-03-22T00:54:47.170180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='iphone 15')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = get_text_chunks('iphone 15')\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3115ed4-608f-4635-b856-0b36df76dffa",
   "metadata": {},
   "source": [
    "### QA Retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4ae4c62-9cdf-4725-9f07-9a034607ddd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T23:49:05.562558Z",
     "iopub.status.busy": "2024-03-21T23:49:05.561988Z",
     "iopub.status.idle": "2024-03-21T23:49:05.598380Z",
     "shell.execute_reply": "2024-03-21T23:49:05.597439Z",
     "shell.execute_reply.started": "2024-03-21T23:49:05.562514Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.retrieval_qa.base import BaseRetrievalQA\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "OPENAI_MODEL = 'gpt-3.5-turbo'\n",
    "BASE_PERSIST_DIRECTORY = '../db/chroma_3/'\n",
    "\n",
    "\n",
    "def embed_texts(texts, model_name) -> list[Document]:\n",
    "    if model_name == 'openai':\n",
    "        embedder = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "    else:\n",
    "        embedder = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        \n",
    "    vector_db = Chroma.from_documents(\n",
    "        texts, \n",
    "        embedder,\n",
    "        persist_directory=BASE_PERSIST_DIRECTORY + model_name\n",
    "    )\n",
    "    return vector_db\n",
    "\n",
    "def get_qa(model, vector_db) -> BaseRetrievalQA:\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_db.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    return_source_documents=False,\n",
    "    verbose=False,\n",
    "    )\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55523e-2e99-45ec-aef0-55fe36910208",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e04ef45-bd84-444c-b47b-d344e409ae4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:00:50.214755Z",
     "iopub.status.busy": "2024-03-22T00:00:50.214047Z",
     "iopub.status.idle": "2024-03-22T00:00:52.660762Z",
     "shell.execute_reply": "2024-03-22T00:00:52.659533Z",
     "shell.execute_reply.started": "2024-03-22T00:00:50.214708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x10a235db0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12ee8d1e0> openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x10a235db0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12ee8d1e0>, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12ee5eb60>, search_kwargs={'k': 1})\n",
      "1. Coverage\n",
      "2. Longevity\n",
      "3. Ease of application\n",
      "4. Shade range\n",
      "5. Hydration\n",
      "6. Packaging\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_openai = ChatOpenAI(model_name=OPENAI_MODEL, openai_api_key=OPENAI_API_KEY)\n",
    "print(llm_openai, '\\n')\n",
    "\n",
    "vector_db = embed_texts(texts, model_name='openai')\n",
    "qa_openai = get_qa(llm_openai, vector_db)\n",
    "print(qa_openai, '\\n')\n",
    "\n",
    "answer = qa_openai.invoke(prompt_text)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6537790c-04f2-460d-a99a-2e77632115d0",
   "metadata": {},
   "source": [
    "#### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b69ce2f-6808-4c7f-9b1a-3cdbff72bb9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T23:50:47.828471Z",
     "iopub.status.busy": "2024-03-21T23:50:47.827596Z",
     "iopub.status.idle": "2024-03-21T23:50:48.321404Z",
     "shell.execute_reply": "2024-03-21T23:50:48.320148Z",
     "shell.execute_reply.started": "2024-03-21T23:50:47.828407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Resuming transfer from byte position 4108928128\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 27242    0 27242    0     0    97k      0 --:--:-- --:--:-- --:--:--   97k\n"
     ]
    }
   ],
   "source": [
    "HG_EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "MODEL_NAME = 'mistral-7b-openorca.gguf2.Q4_0.gguf'  # GPT4All model\n",
    "MODEL_PATH = '../models/' + MODEL_NAME\n",
    "!curl -C - -o {MODEL_PATH} https://gpt4all.io/models/gguf/{MODEL_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "798bffaa-d34f-4707-aab2-065e94aac2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:22:20.408407Z",
     "iopub.status.busy": "2024-03-22T00:22:20.408052Z",
     "iopub.status.idle": "2024-03-22T00:23:09.805889Z",
     "shell.execute_reply": "2024-03-22T00:23:09.805028Z",
     "shell.execute_reply.started": "2024-03-22T00:22:20.408381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 149 ms, sys: 675 ms, total: 824 ms\n",
      "Wall time: 865 ms\n",
      "\u001b[1mGPT4All\u001b[0m\n",
      "Params: {'model': '../models/mistral-7b-openorca.gguf2.Q4_0.gguf', 'max_tokens': 200, 'n_predict': 256, 'top_k': 40, 'top_p': 0.1, 'temp': 0.7, 'n_batch': 8, 'repeat_penalty': 1.18, 'repeat_last_n': 64, 'streaming': False} \n",
      "\n",
      "CPU times: user 900 ms, sys: 244 ms, total: 1.14 s\n",
      "Wall time: 2.27 s\n",
      "combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=GPT4All(verbose=True, callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x10adaae60>], model='../models/mistral-7b-openorca.gguf2.Q4_0.gguf', client=<gpt4all.gpt4all.GPT4All object at 0x12f92d8a0>)), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12ee78610>, search_kwargs={'k': 1}) \n",
      "\n",
      "CPU times: user 2min 41s, sys: 1.78 s, total: 2min 43s\n",
      "Wall time: 44.2 s\n",
      "\n",
      "1. Effectiveness of the concealer on dark circles\n",
      "2. Ease of application\n",
      "3. Staying power (how long it lasts)\n",
      "4. Skin compatibility and comfort\n",
      "5. Shade range available\n",
      "6. Price point\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.streaming_stdout_final_only import FinalStreamingStdOutCallbackHandler\n",
    "from langchain.llms import GPT4All\n",
    "\n",
    "callbacks = [FinalStreamingStdOutCallbackHandler()]\n",
    "llm_local = %time GPT4All(model=MODEL_PATH, callbacks=callbacks, verbose=True)\n",
    "print(llm_local, '\\n')\n",
    "\n",
    "vector_db = %time embed_texts(texts, model_name=HG_EMBEDDING_MODEL)\n",
    "qa_local = get_qa(llm_local, vector_db)\n",
    "print(qa_local, '\\n')\n",
    "\n",
    "answer = %time qa_local.invoke(prompt_text)\n",
    "print(answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506cae0-7f87-4ad9-9805-3154ec6c4a1d",
   "metadata": {},
   "source": [
    "### LLM Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0bb19559-0014-4944-a951-21bc99f2fd30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:23:09.808362Z",
     "iopub.status.busy": "2024-03-22T00:23:09.807800Z",
     "iopub.status.idle": "2024-03-22T00:23:09.868238Z",
     "shell.execute_reply": "2024-03-22T00:23:09.867151Z",
     "shell.execute_reply.started": "2024-03-22T00:23:09.808328Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_text2 = \"Please list between 3 and 6 criteria that could be rated for this product. Just return the list of criteria, nothing more.\"\n",
    "template = f\"Product: '{{product_text}}'\\n{prompt_text2}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"product_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd00af6b-0d78-4645-b568-0dc60836e42a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:23:09.869911Z",
     "iopub.status.busy": "2024-03-22T00:23:09.869540Z",
     "iopub.status.idle": "2024-03-22T00:24:23.754181Z",
     "shell.execute_reply": "2024-03-22T00:24:23.753311Z",
     "shell.execute_reply.started": "2024-03-22T00:23:09.869880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 176 µs, sys: 2 µs, total: 178 µs\n",
      "Wall time: 183 µs\n",
      "prompt=PromptTemplate(input_variables=['product_text'], template=\"Product: '{product_text}'\\nPlease list between 3 and 6 criteria that could be rated for this product. Just return the list of criteria, nothing more.\") llm=GPT4All(verbose=True, callbacks=[<langchain.callbacks.streaming_stdout_final_only.FinalStreamingStdOutCallbackHandler object at 0x10adaae60>], model='../models/mistral-7b-openorca.gguf2.Q4_0.gguf', client=<gpt4all.gpt4all.GPT4All object at 0x12f92d8a0>) \n",
      "\n",
      "CPU times: user 4min 45s, sys: 2.85 s, total: 4min 48s\n",
      "Wall time: 1min 13s\n",
      "\n",
      "1. Coverage\n",
      "2. Ease of application\n",
      "3. Blendability\n",
      "4. Longevity\n",
      "5. Skin compatibility\n",
      "6. Price\n",
      "\n",
      "Please provide a brief description of each criterion:\n",
      "\n",
      "1. Coverage - The amount of product needed to effectively cover dark circles and imperfections on the skin.\n",
      "2. Ease of application - How easily the concealer can be applied, including its consistency and packaging design.\n",
      "3. Blendability - How well the concealer blends with the user's natural skin tone for a seamless finish.\n",
      "4. Longevity - The duration that the product remains effective on the skin before needing to be reapplied or showing signs of wear.\n",
      "5. Skin compatibility - Whether the product is suitable for all skin types, including those with sensitive skin and potential allergies.\n",
      "6. Price - The cost-effectiveness of the concealer in comparison to its performance and competitors' products.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "llm_chain = %time LLMChain(prompt=prompt, llm=llm_local, return_final_only=True)\n",
    "print(llm_chain, '\\n')\n",
    "\n",
    "res = %time llm_chain.run(product_text=PRODUCT_INPUT)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7ac7bae-7d13-4fa0-83de-610591d8c45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-22T00:25:06.708548Z",
     "iopub.status.busy": "2024-03-22T00:25:06.707828Z",
     "iopub.status.idle": "2024-03-22T00:25:07.784671Z",
     "shell.execute_reply": "2024-03-22T00:25:07.783328Z",
     "shell.execute_reply.started": "2024-03-22T00:25:06.708500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. Coverage\\n2. Longevity\\n3. Shade range\\n4. Consistency\\n5. Ease of application\\n6. Anti-aging benefits'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "\n",
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, return_final_only=True)\n",
    "llm_chain.run(product_text=PRODUCT_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b770ef-2a69-4ea8-b283-29fd2894722e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
